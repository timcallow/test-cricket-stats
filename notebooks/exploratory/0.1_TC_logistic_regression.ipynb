{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc6b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by loading the key packages\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf2c267",
   "metadata": {},
   "source": [
    "First, let us load in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataframe\n",
    "df = pd.read_csv(\"../../data/processed/aggregate_data.csv\")\n",
    "# make a copy for edits\n",
    "df_copy = df\n",
    "df_copy = df_copy.drop(columns=[\"Unnamed: 0\"])\n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a629f84",
   "metadata": {},
   "source": [
    "We are first going to split the dataframe into training and test sets (in the ratio 80:20). The test set will be used later, to compare the accuracy of different ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the training and test sets\n",
    "df_train = df_copy.sample(frac=0.8, ignore_index=True)\n",
    "df_test = df_copy.drop(df_train.index)\n",
    "\n",
    "# save them for later comparisons\n",
    "df_train.to_csv('../../data/processed/training_data.csv')\n",
    "df_test.to_csv('../../data/processed/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351483b4",
   "metadata": {},
   "source": [
    "In the next step, we shall map the data into appropriate values. We shall assign labels of 1,0,-1 for a home win / draw / away win. For the toss, we use 1/-1 for home toss / away toss. \n",
    "\n",
    "We shall furthermore introduce two new variables, `rank_diff=away_rank-home_rank` and `rating_diff=home_rating-away_rating`. These are a bit more simple to understand than having two separate ratings and rankings for the different teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d03dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the mapping\n",
    "toss_dict = {\"home\": 1, \"away\": -1}\n",
    "result_dict = {\"home\": 1, \"draw\": 0, \"away\": -1}\n",
    "# map the data\n",
    "result_map = [result_dict[result] for result in df_train.result]\n",
    "toss_map = [toss_dict[toss] for toss in df_train.toss]\n",
    "# introduce rank_diff and rating_diff\n",
    "df_train[\"rank_diff\"] = df_train.away_rank - df_train.home_rank\n",
    "df_train[\"rating_diff\"] = df_train.home_rating - df_train.away_rating\n",
    "# change the dataframe\"\n",
    "df_train.result = result_map\n",
    "df_train.toss = toss_map\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44bc8e",
   "metadata": {},
   "source": [
    "In the next step we pre-process the features to have mean of zero and standard deviation of unity. We shall only use three features in the first instance, toss, rankings_diff and ratings_diff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42124345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "feature_cols = [\"rank_diff\",\"rating_diff\",\"toss\"]\n",
    "X_init = df_train[feature_cols]\n",
    "scaler = preprocessing.StandardScaler().fit(X_init)\n",
    "X_scaled = scaler.transform(X_init)\n",
    "Y = df_train.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f566250",
   "metadata": {},
   "source": [
    "The next step is to split our data into training and validation sets. We shall use $k$-fold cross-validation with $k=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eac070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6835eb9",
   "metadata": {},
   "source": [
    "Now we initialize the logistic regression model. First up we shall just try with default mulitnomial regression from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec07b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594e07ee",
   "metadata": {},
   "source": [
    "Next up we shall train our model using the 5-fold cross validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "logreg_score = np.zeros((5))\n",
    "\n",
    "for i, [train_index, val_index] in enumerate(kf.split(X_init)):\n",
    "            \n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "    \n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_val)\n",
    "\n",
    "    logreg_score[i] = logreg.score(X_val, Y_val)\n",
    "    print(i, logreg_score[i])\n",
    "    \n",
    "print(np.mean(logreg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28fc58",
   "metadata": {},
   "source": [
    "The accuracy is not particularly good! In fact, if we were to naively assign the the home team to always win, we would get an accuracy of 139/296=0.47, so we are hardly improving on the naive prediction...\n",
    "\n",
    "Let us try to introduce some new parameters based on the teams playing. To do this, we need to transform the categorical team labels into binary arrays, which we shall do via a \"one hot encoder\" approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04341af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data set wasn't modified by mistake\n",
    "df_train.head()\n",
    "\n",
    "# drop the date\n",
    "df_train = df_train.drop(columns=[\"date\"])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dummy variables\n",
    "df_dummy = pd.get_dummies(df_train, prefix=['ht', 'at'])\n",
    "\n",
    "df_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the training set equal to the dummy set\n",
    "df_train = df_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of the columns\n",
    "col_list = df_train.columns.to_list()\n",
    "for a in [\"result\",\"home_rank\",\"away_rank\",\"home_rating\", \"away_rating\"]:\n",
    "    col_list.remove(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_cols = col_list\n",
    "feature_cols = [\"rank_diff\"]\n",
    "print(feature_cols)\n",
    "X_init = df_train[feature_cols]\n",
    "scaler = preprocessing.StandardScaler().fit(X_init)\n",
    "X_scaled = scaler.transform(X_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=0.1,tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73cbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "logreg_score = np.zeros((5))\n",
    "\n",
    "for i, [train_index, val_index] in enumerate(kf.split(X_init)):\n",
    "            \n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "    \n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_val)\n",
    "\n",
    "    logreg_score[i] = logreg.score(X_val, Y_val)\n",
    "    print(i+1, logreg_score[i])\n",
    "\n",
    "y_test_pred = logreg.predict(X_train)\n",
    "print(logreg.score(X_train,Y_train))\n",
    "print(np.mean(logreg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e557dc",
   "metadata": {},
   "source": [
    "We will simplify the problem by excluding drawn results for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e951d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(df_train[df.result == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ade299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50eaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = col_list\n",
    "#feature_cols = [\"rank_diff\", \"toss\", \"rating_diff\"]\n",
    "X_init = df_train[feature_cols]\n",
    "scaler = preprocessing.StandardScaler().fit(X_init)\n",
    "X_scaled = scaler.transform(X_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07099daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_score = np.zeros((5))\n",
    "\n",
    "for i, [train_index, val_index] in enumerate(kf.split(X_init)):\n",
    "            \n",
    "    X_train, X_val = X_scaled[train_index], X_scaled[val_index]\n",
    "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "    \n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_val)\n",
    "\n",
    "    logreg_score[i] = logreg.score(X_val, Y_val)\n",
    "    print(i+1, logreg_score[i], logreg.score(X_train, Y_train))\n",
    "\n",
    "y_test_pred = logreg.predict(X_train)\n",
    "print(np.mean(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e6903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
